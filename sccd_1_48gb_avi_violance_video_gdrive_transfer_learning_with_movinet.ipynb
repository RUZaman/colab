{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RUZaman/colab/blob/main/sccd_1_48gb_avi_violance_video_gdrive_transfer_learning_with_movinet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "data from kaggle https://www.kaggle.com/datasets/mohamedmustafa/real-life-violence-situations-dataset/code used here "
      ],
      "metadata": {
        "id": "YrFihvu9nqqf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOgDUDMAG6mn"
      },
      "source": [
        "##### Copyright 2022 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.tensorflow.org/tutorials/video/transfer_learning_with_movinet"
      ],
      "metadata": {
        "id": "Cfz5FzREnp5f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3PsBDmGG_W8",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XMtM0UYxnOB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SegLDVconTb3",
        "outputId": "6f6e1e71-e0d6-4af9-d9ec-2e1a60c8ed61"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/gdrive/My Drive/foo.txt', 'w') as f:\n",
        "  f.write('Hello Google Drive!')\n",
        "!cat '/gdrive/My Drive/foo.txt'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfb8wgYNnTb7",
        "outputId": "09a664c3-6089-4ecb-fa5e-9b278d049de7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello Google Drive!"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifkGYxdCHIof"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/video/transfer_learning_with_movinet\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/video/transfer_learning_with_movinet.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/video/transfer_learning_with_movinet.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/video/transfer_learning_with_movinet.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1iXMLGan85W",
        "outputId": "4efe4495-3e61-4cb5-f14a-b8378d62a740"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RC7HnKeNoNcb",
        "outputId": "bf49faf1-9928-476a-9f5b-a2cfd7470be4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/movinet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58-Ke9dkoR4k",
        "outputId": "1c23d9ce-4160-49c0-c442-5698708c588d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/movinet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmdjQJTdoPUJ",
        "outputId": "7a0cc8c7-a08f-4150-d3e1-969021a74525"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/movinet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWxDDkRwLVMC"
      },
      "source": [
        "# Transfer learning for video classification with MoViNet\n",
        "\n",
        "MoViNets (Mobile Video Networks) provide a family of efficient video classification models, supporting inference on streaming video. In this tutorial, you will use a pre-trained MoViNet model to classify videos, specifically for an action recognition task, from the [UCF101 dataset](https://www.crcv.ucf.edu/data/UCF101.php). A pre-trained model is a saved network that was previously trained on a larger dataset. You can find more details about MoViNets in the [MoViNets: Mobile Video Networks for Efficient Video Recognition](https://arxiv.org/abs/2103.11511) paper by Kondratyuk, D. et al. (2021). In this tutorial, you will: \n",
        "\n",
        "* Learn how to download a pre-trained MoViNet model\n",
        "* Create a new model using a pre-trained model with a new classifier by freezing the convolutional base of the MoViNet model\n",
        "* Replace the classifier head with the number of labels of a new dataset\n",
        "* Perform transfer learning on the [UCF101 dataset](https://www.crcv.ucf.edu/data/UCF101.php)\n",
        "\n",
        "The model downloaded in this tutorial is from [official/projects/movinet](https://github.com/tensorflow/models/tree/master/official/projects/movinet). This repository contains a collection of MoViNet models that TF Hub uses in the TensorFlow 2 SavedModel format.\n",
        "\n",
        "This transfer learning tutorial is the third part in a series of TensorFlow video tutorials. Here are the other three tutorials:\n",
        "\n",
        "- [Load video data](https://www.tensorflow.org/tutorials/load_data/video): This tutorial explains much of the code used in this document; in particular, how to preprocess and load data through the `FrameGenerator` class is explained in more detail.\n",
        "- [Build a 3D CNN model for video classification](https://www.tensorflow.org/tutorials/video/video_classification). Note that this tutorial uses a (2+1)D CNN that decomposes the spatial and temporal aspects of 3D data; if you are using volumetric data such as an MRI scan, consider using a 3D CNN instead of a (2+1)D CNN.\n",
        "- [MoViNet for streaming action recognition](https://www.tensorflow.org/hub/tutorials/movinet): Get familiar with the MoViNet models that are available on TF Hub."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GidiisyXwK--"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Begin by installing and importing some necessary libraries, including:\n",
        "[remotezip](https://github.com/gtsystem/python-remotezip) to inspect the contents of a ZIP file, [tqdm](https://github.com/tqdm/tqdm) to use a progress bar, [OpenCV](https://opencv.org/) to process video files (ensure that `opencv-python` and `opencv-python-headless` are the same version), and TensorFlow models ([`tf-models-official`](https://github.com/tensorflow/models/tree/master/official)) to download the pre-trained MoViNet model. The TensorFlow models package are a collection of models that use TensorFlow’s high-level APIs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "nubWhqYdwEXD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8455ae5b-51b3-470d-dcac-9f7c85b7a5ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting remotezip\n",
            "  Downloading remotezip-0.12.1.tar.gz (7.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (4.64.1)\n",
            "Collecting opencv-python==4.5.2.52\n",
            "  Downloading opencv_python-4.5.2.52-cp38-cp38-manylinux2014_x86_64.whl (51.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opencv-python-headless==4.5.2.52\n",
            "  Downloading opencv_python_headless-4.5.2.52-cp38-cp38-manylinux2014_x86_64.whl (38.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.2/38.2 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tf-models-official\n",
            "  Downloading tf_models_official-2.11.3-py2.py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m97.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from opencv-python==4.5.2.52) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from remotezip) (2.25.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.8/dist-packages (from remotezip) (0.8.10)\n",
            "Requirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tf-models-official) (0.12.0)\n",
            "Requirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.8/dist-packages (from tf-models-official) (1.3.5)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.8/dist-packages (from tf-models-official) (2.0.6)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.8/dist-packages (from tf-models-official) (0.29.33)\n",
            "Collecting tensorflow-model-optimization>=0.4.1\n",
            "  Downloading tensorflow_model_optimization-0.7.3-py2.py3-none-any.whl (238 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.9/238.9 KB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.8/dist-packages (from tf-models-official) (1.5.12)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from tf-models-official) (1.15.0)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.8/dist-packages (from tf-models-official) (0.5.0)\n",
            "Requirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.8/dist-packages (from tf-models-official) (2.70.0)\n",
            "Collecting immutabledict\n",
            "  Downloading immutabledict-2.2.3-py3-none-any.whl (4.0 kB)\n",
            "Collecting pyyaml<6.0,>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m662.4/662.4 KB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow~=2.11.0\n",
            "  Downloading tensorflow-2.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.19.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sacrebleu\n",
            "  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 KB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: oauth2client in /usr/local/lib/python3.8/dist-packages (from tf-models-official) (4.1.3)\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 KB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from tf-models-official) (3.2.2)\n",
            "Collecting tf-slim>=1.1.0\n",
            "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m352.1/352.1 KB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.8/dist-packages (from tf-models-official) (4.8.1)\n",
            "Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.8/dist-packages (from tf-models-official) (5.4.8)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.8/dist-packages (from tf-models-official) (1.7.3)\n",
            "Collecting py-cpuinfo>=3.3.0\n",
            "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from tf-models-official) (7.1.2)\n",
            "Collecting tensorflow-text~=2.11.0\n",
            "  Downloading tensorflow_text-2.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m102.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.8/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (4.1.1)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=1.19.0 in /usr/local/lib/python3.8/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (2.16.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.8/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (2.11.0)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.8/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (0.17.4)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /usr/local/lib/python3.8/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (0.1.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from kaggle>=1.3.9->tf-models-official) (2022.12.7)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.8/dist-packages (from kaggle>=1.3.9->tf-models-official) (2.8.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.8/dist-packages (from kaggle>=1.3.9->tf-models-official) (1.24.3)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.8/dist-packages (from kaggle>=1.3.9->tf-models-official) (7.0.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.22.0->tf-models-official) (2022.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official) (57.4.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official) (15.0.6.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official) (1.14.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official) (2.2.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official) (1.6.3)\n",
            "Collecting tensorboard<2.12,>=2.11\n",
            "  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m110.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official) (1.3.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official) (0.4.0)\n",
            "Collecting flatbuffers>=2.0\n",
            "  Downloading flatbuffers-23.1.21-py2.py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official) (0.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official) (4.4.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official) (3.19.6)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official) (0.29.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official) (1.51.1)\n",
            "Collecting keras<2.12,>=2.11.0\n",
            "  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<2.12,>=2.11.0\n",
            "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.2/439.2 KB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official) (3.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official) (21.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official) (3.3.0)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official) (0.1.8)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->tf-models-official) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->tf-models-official) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->tf-models-official) (3.0.9)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from oauth2client->tf-models-official) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.8/dist-packages (from oauth2client->tf-models-official) (0.2.8)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.8/dist-packages (from oauth2client->tf-models-official) (0.4.8)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->remotezip) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->remotezip) (2.10)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from sacrebleu->tf-models-official) (2022.6.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.8/dist-packages (from sacrebleu->tf-models-official) (4.9.2)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.8/dist-packages (from seqeval->tf-models-official) (1.0.2)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.8/dist-packages (from tensorflow-addons->tf-models-official) (2.7.1)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets->tf-models-official) (0.10.2)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets->tf-models-official) (1.12.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets->tf-models-official) (5.10.2)\n",
            "Requirement already satisfied: etils[enp,epath]>=0.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets->tf-models-official) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets->tf-models-official) (7.1.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets->tf-models-official) (0.3.6)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets->tf-models-official) (2.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow~=2.11.0->tf-models-official) (0.38.4)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.8/dist-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets->tf-models-official) (3.11.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.8/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.6.7->tf-models-official) (1.58.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client>=1.6.7->tf-models-official) (5.2.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official) (1.8.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.8/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-official) (1.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official) (6.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official) (3.2.2)\n",
            "Building wheels for collected packages: remotezip, seqeval\n",
            "  Building wheel for remotezip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for remotezip: filename=remotezip-0.12.1-py3-none-any.whl size=7947 sha256=ae7da3c522bf3d2e947b1f67a6587b9e457d92504deebc66eab8408b6f624714\n",
            "  Stored in directory: /root/.cache/pip/wheels/36/69/50/7b5a7fd4fda1cbb85c080b1c05cbbd2f88ac6a665260910b13\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16179 sha256=bb8e87139f89ffc8fe45e41042fd4c8bb7b33ed01dcda0742dab6ed9ed1c9d3f\n",
            "  Stored in directory: /root/.cache/pip/wheels/ad/5c/ba/05fa33fa5855777b7d686e843ec07452f22a66a138e290e732\n",
            "Successfully built remotezip seqeval\n",
            "Installing collected packages: sentencepiece, py-cpuinfo, flatbuffers, tf-slim, tensorflow-model-optimization, tensorflow-estimator, pyyaml, portalocker, opencv-python-headless, opencv-python, keras, immutabledict, colorama, tensorflow-addons, sacrebleu, remotezip, seqeval, tensorboard, tensorflow, tensorflow-text, tf-models-official\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 1.12\n",
            "    Uninstalling flatbuffers-1.12:\n",
            "      Successfully uninstalled flatbuffers-1.12\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.9.0\n",
            "    Uninstalling tensorflow-estimator-2.9.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.9.0\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 6.0\n",
            "    Uninstalling PyYAML-6.0:\n",
            "      Successfully uninstalled PyYAML-6.0\n",
            "  Attempting uninstall: opencv-python-headless\n",
            "    Found existing installation: opencv-python-headless 4.7.0.68\n",
            "    Uninstalling opencv-python-headless-4.7.0.68:\n",
            "      Successfully uninstalled opencv-python-headless-4.7.0.68\n",
            "  Attempting uninstall: opencv-python\n",
            "    Found existing installation: opencv-python 4.6.0.66\n",
            "    Uninstalling opencv-python-4.6.0.66:\n",
            "      Successfully uninstalled opencv-python-4.6.0.66\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.9.0\n",
            "    Uninstalling keras-2.9.0:\n",
            "      Successfully uninstalled keras-2.9.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.9.1\n",
            "    Uninstalling tensorboard-2.9.1:\n",
            "      Successfully uninstalled tensorboard-2.9.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.9.2\n",
            "    Uninstalling tensorflow-2.9.2:\n",
            "      Successfully uninstalled tensorflow-2.9.2\n",
            "Successfully installed colorama-0.4.6 flatbuffers-23.1.21 immutabledict-2.2.3 keras-2.11.0 opencv-python-4.5.2.52 opencv-python-headless-4.5.2.52 portalocker-2.7.0 py-cpuinfo-9.0.0 pyyaml-5.4.1 remotezip-0.12.1 sacrebleu-2.3.1 sentencepiece-0.1.97 seqeval-1.2.2 tensorboard-2.11.2 tensorflow-2.11.0 tensorflow-addons-0.19.0 tensorflow-estimator-2.11.0 tensorflow-model-optimization-0.7.3 tensorflow-text-2.11.0 tf-models-official-2.11.3 tf-slim-1.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install remotezip tqdm opencv-python==4.5.2.52 opencv-python-headless==4.5.2.52 tf-models-official"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "QImPsudoK9JI"
      },
      "outputs": [],
      "source": [
        "import tqdm\n",
        "import random\n",
        "import pathlib\n",
        "import itertools\n",
        "import collections\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import remotezip as rz\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "\n",
        "# Import the MoViNet model from TensorFlow Models (tf-models-official) for the MoViNet model\n",
        "from official.projects.movinet.modeling import movinet\n",
        "from official.projects.movinet.modeling import movinet_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2w3H4dfOPfnm"
      },
      "source": [
        "## Load data\n",
        " \n",
        "The hidden cell below defines helper functions to download a slice of data from the UCF-101 dataset, and load it into a `tf.data.Dataset`. The [Loading video data tutorial](https://www.tensorflow.org/tutorials/load_data/video) provides a detailed walkthrough of this code.\n",
        "\n",
        "The `FrameGenerator` class at the end of the hidden block is the most important utility here. It creates an iterable object that can feed data into the TensorFlow data pipeline. Specifically, this class contains a Python generator that loads the video frames along with its encoded label. The generator (`__call__`) function yields the frame array produced by `frames_from_video_file` and a one-hot encoded vector of the label associated with the set of frames.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "fwEhJ13_PSy6"
      },
      "outputs": [],
      "source": [
        "#@title \n",
        "\n",
        "def list_files_per_class(zip_url):\n",
        "  \"\"\"\n",
        "    List the files in each class of the dataset given the zip URL.\n",
        "\n",
        "    Args:\n",
        "      zip_url: URL from which the files can be unzipped. \n",
        "\n",
        "    Return:\n",
        "      files: List of files in each of the classes.\n",
        "  \"\"\"\n",
        "  files = []\n",
        "  with rz.RemoteZip(URL) as zip:\n",
        "    for zip_info in zip.infolist():\n",
        "      files.append(zip_info.filename)\n",
        "  return files\n",
        "\n",
        "def get_class(fname):\n",
        "  \"\"\"\n",
        "    Retrieve the name of the class given a filename.\n",
        "\n",
        "    Args:\n",
        "      fname: Name of the file in the UCF101 dataset.\n",
        "\n",
        "    Return:\n",
        "      Class that the file belongs to.\n",
        "  \"\"\"\n",
        "  return fname.split('_')[-3]\n",
        "\n",
        "def get_files_per_class(files):\n",
        "  \"\"\"\n",
        "    Retrieve the files that belong to each class. \n",
        "\n",
        "    Args:\n",
        "      files: List of files in the dataset.\n",
        "\n",
        "    Return:\n",
        "      Dictionary of class names (key) and files (values).\n",
        "  \"\"\"\n",
        "  files_for_class = collections.defaultdict(list)\n",
        "  for fname in files:\n",
        "    class_name = get_class(fname)\n",
        "    files_for_class[class_name].append(fname)\n",
        "  return files_for_class\n",
        "\n",
        "def download_from_zip(zip_url, to_dir, file_names):\n",
        "  \"\"\"\n",
        "    Download the contents of the zip file from the zip URL.\n",
        "\n",
        "    Args:\n",
        "      zip_url: Zip URL containing data.\n",
        "      to_dir: Directory to download data to.\n",
        "      file_names: Names of files to download.\n",
        "  \"\"\"\n",
        "  with rz.RemoteZip(zip_url) as zip:\n",
        "    for fn in tqdm.tqdm(file_names):\n",
        "      class_name = get_class(fn)\n",
        "      zip.extract(fn, str(to_dir / class_name))\n",
        "      unzipped_file = to_dir / class_name / fn\n",
        "\n",
        "      fn = pathlib.Path(fn).parts[-1]\n",
        "      output_file = to_dir / class_name / fn\n",
        "      unzipped_file.rename(output_file,)\n",
        "\n",
        "def split_class_lists(files_for_class, count):\n",
        "  \"\"\"\n",
        "    Returns the list of files belonging to a subset of data as well as the remainder of\n",
        "    files that need to be downloaded.\n",
        "\n",
        "    Args:\n",
        "      files_for_class: Files belonging to a particular class of data.\n",
        "      count: Number of files to download.\n",
        "\n",
        "    Return:\n",
        "      split_files: Files belonging to the subset of data.\n",
        "      remainder: Dictionary of the remainder of files that need to be downloaded.\n",
        "  \"\"\"\n",
        "  split_files = []\n",
        "  remainder = {}\n",
        "  for cls in files_for_class:\n",
        "    split_files.extend(files_for_class[cls][:count])\n",
        "    remainder[cls] = files_for_class[cls][count:]\n",
        "  return split_files, remainder\n",
        "\n",
        "def download_ufc_101_subset(zip_url, num_classes, splits, download_dir):\n",
        "  \"\"\"\n",
        "    Download a subset of the UFC101 dataset and split them into various parts, such as\n",
        "    training, validation, and test. \n",
        "\n",
        "    Args:\n",
        "      zip_url: Zip URL containing data.\n",
        "      num_classes: Number of labels.\n",
        "      splits: Dictionary specifying the training, validation, test, etc. (key) division of data \n",
        "              (value is number of files per split).\n",
        "      download_dir: Directory to download data to.\n",
        "\n",
        "    Return:\n",
        "      dir: Posix path of the resulting directories containing the splits of data.\n",
        "  \"\"\"\n",
        "  files = list_files_per_class(zip_url)\n",
        "  for f in files:\n",
        "    tokens = f.split('/')\n",
        "    if len(tokens) <= 2:\n",
        "      files.remove(f) # Remove that item from the list if it does not have a filename\n",
        "\n",
        "  files_for_class = get_files_per_class(files)\n",
        "\n",
        "  classes = list(files_for_class.keys())[:num_classes]\n",
        "\n",
        "  for cls in classes:\n",
        "    new_files_for_class = files_for_class[cls]\n",
        "    random.shuffle(new_files_for_class)\n",
        "    files_for_class[cls] = new_files_for_class\n",
        "\n",
        "  # Only use the number of classes you want in the dictionary\n",
        "  files_for_class = {x: files_for_class[x] for x in list(files_for_class)[:num_classes]}\n",
        "\n",
        "  dirs = {}\n",
        "  for split_name, split_count in splits.items():\n",
        "    print(split_name, \":\")\n",
        "    split_dir = download_dir / split_name\n",
        "    split_files, files_for_class = split_class_lists(files_for_class, split_count)\n",
        "    download_from_zip(zip_url, split_dir, split_files)\n",
        "    dirs[split_name] = split_dir\n",
        "\n",
        "  return dirs\n",
        "\n",
        "def format_frames(frame, output_size):\n",
        "  \"\"\"\n",
        "    Pad and resize an image from a video.\n",
        "\n",
        "    Args:\n",
        "      frame: Image that needs to resized and padded. \n",
        "      output_size: Pixel size of the output frame image.\n",
        "\n",
        "    Return:\n",
        "      Formatted frame with padding of specified output size.\n",
        "  \"\"\"\n",
        "  frame = tf.image.convert_image_dtype(frame, tf.float32)\n",
        "  frame = tf.image.resize_with_pad(frame, *output_size)\n",
        "  return frame\n",
        "\n",
        "def frames_from_video_file(video_path, n_frames, output_size = (224,224), frame_step = 15):\n",
        "  \"\"\"\n",
        "    Creates frames from each video file present for each category.\n",
        "\n",
        "    Args:\n",
        "      video_path: File path to the video.\n",
        "      n_frames: Number of frames to be created per video file.\n",
        "      output_size: Pixel size of the output frame image.\n",
        "\n",
        "    Return:\n",
        "      An NumPy array of frames in the shape of (n_frames, height, width, channels).\n",
        "  \"\"\"\n",
        "  # Read each video frame by frame\n",
        "  result = []\n",
        "  src = cv2.VideoCapture(str(video_path))  \n",
        "\n",
        "  video_length = src.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "\n",
        "  need_length = 1 + (n_frames - 1) * frame_step\n",
        "\n",
        "  if need_length > video_length:\n",
        "    start = 0\n",
        "  else:\n",
        "    max_start = video_length - need_length\n",
        "    start = random.randint(0, max_start + 1)\n",
        "\n",
        "  src.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
        "  # ret is a boolean indicating whether read was successful, frame is the image itself\n",
        "  ret, frame = src.read()\n",
        "  result.append(format_frames(frame, output_size))\n",
        "\n",
        "  for _ in range(n_frames - 1):\n",
        "    for _ in range(frame_step):\n",
        "      ret, frame = src.read()\n",
        "    if ret:\n",
        "      frame = format_frames(frame, output_size)\n",
        "      result.append(frame)\n",
        "    else:\n",
        "      result.append(np.zeros_like(result[0]))\n",
        "  src.release()\n",
        "  result = np.array(result)[..., [2, 1, 0]]\n",
        "\n",
        "  return result\n",
        "\n",
        "class FrameGenerator:\n",
        "  def __init__(self, path, n_frames, training = False):\n",
        "    \"\"\" Returns a set of frames with their associated label. \n",
        "\n",
        "      Args:\n",
        "        path: Video file paths.\n",
        "        n_frames: Number of frames. \n",
        "        training: Boolean to determine if training dataset is being created.\n",
        "    \"\"\"\n",
        "    self.path = path\n",
        "    self.n_frames = n_frames\n",
        "    self.training = training\n",
        "    self.class_names = sorted(set(p.name for p in self.path.iterdir() if p.is_dir()))\n",
        "    self.class_ids_for_name = dict((name, idx) for idx, name in enumerate(self.class_names))\n",
        "\n",
        "  def get_files_and_class_names(self):\n",
        "    video_paths = list(self.path.glob('*/*.avi'))\n",
        "    classes = [p.parent.name for p in video_paths] \n",
        "    return video_paths, classes\n",
        "\n",
        "  def __call__(self):\n",
        "    video_paths, classes = self.get_files_and_class_names()\n",
        "\n",
        "    pairs = list(zip(video_paths, classes))\n",
        "\n",
        "    if self.training:\n",
        "      random.shuffle(pairs)\n",
        "\n",
        "    for path, name in pairs:\n",
        "      video_frames = frames_from_video_file(path, self.n_frames) \n",
        "      label = self.class_ids_for_name[name] # Encode labels\n",
        "      yield video_frames, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "vDHrNLZkPSR9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4797c869-f72d-4362-ff56-5fe285bbc93a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train :\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 60/60 [00:08<00:00,  6.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test :\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:03<00:00, 11.02it/s]\n"
          ]
        }
      ],
      "source": [
        "URL = 'https://storage.googleapis.com/thumos14_files/UCF101_videos.zip'\n",
        "download_dir = pathlib.Path('./UCF101_subset/')\n",
        "subset_paths = download_ufc_101_subset(URL, \n",
        "                        num_classes = 2, \n",
        "                        splits = {\"train\": 30, \"test\": 20}, \n",
        "                        download_dir = download_dir)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.test.gpu_device_name()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "quUSmI0asXB2",
        "outputId": "22f80391-86d2-4338-fb5a-77c079501f63"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYYShfhMx9DW"
      },
      "source": [
        "Create the training and test datasets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "-twTu3_Bx-iJ"
      },
      "outputs": [],
      "source": [
        "batch_size = 8\n",
        "num_frames = 8\n",
        "\n",
        "output_signature = (tf.TensorSpec(shape = (None, None, None, 3), dtype = tf.float32),\n",
        "                    tf.TensorSpec(shape = (), dtype = tf.int16))\n",
        "\n",
        "train_ds = tf.data.Dataset.from_generator(FrameGenerator(subset_paths['train'], num_frames, training = True),\n",
        "                                          output_signature = output_signature)\n",
        "train_ds = train_ds.batch(batch_size)\n",
        "\n",
        "test_ds = tf.data.Dataset.from_generator(FrameGenerator(subset_paths['test'], num_frames),\n",
        "                                         output_signature = output_signature)\n",
        "test_ds = test_ds.batch(batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7stgmuBCGQT"
      },
      "source": [
        "The labels generated here represent the encoding of the classes. For instance, 'ApplyEyeMakeup' is mapped to the integer Take a look at the labels of the training data to ensure that the dataset has been sufficiently shuffled. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "k9L2-toXCOQq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b424d13-5061-4dd6-8514-7a16d7175d96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([1 0 0 0 1 0 0 1], shape=(8,), dtype=int16)\n",
            "tf.Tensor([0 1 1 0 1 0 0 0], shape=(8,), dtype=int16)\n",
            "tf.Tensor([0 1 1 0 1 0 0 1], shape=(8,), dtype=int16)\n",
            "tf.Tensor([1 0 0 0 1 0 0 0], shape=(8,), dtype=int16)\n",
            "tf.Tensor([1 0 1 1 0 0 0 1], shape=(8,), dtype=int16)\n",
            "tf.Tensor([0 0 0 0 0 0 0 1], shape=(8,), dtype=int16)\n",
            "tf.Tensor([1 0 1 1 1 1 0 1], shape=(8,), dtype=int16)\n",
            "tf.Tensor([0 1 0 1 0 1 1 0], shape=(8,), dtype=int16)\n",
            "tf.Tensor([0 1 0 1 1 0 0 0], shape=(8,), dtype=int16)\n",
            "tf.Tensor([1 1 0 0 1 1 0 0], shape=(8,), dtype=int16)\n"
          ]
        }
      ],
      "source": [
        "for frames, labels in train_ds.take(10):\n",
        "  print(labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZ3qwZnpfy9c"
      },
      "source": [
        "Take a look at the shape of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "b6MqP4m2fyQT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec4044c4-4bef-4dcc-a307-c910c1d06bb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape: (8, 8, 224, 224, 3)\n",
            "Label: (8,)\n"
          ]
        }
      ],
      "source": [
        "print(f\"Shape: {frames.shape}\")\n",
        "print(f\"Label: {labels.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxbhPqXGvc_F"
      },
      "source": [
        "## What are MoViNets?\n",
        "\n",
        "As mentioned previously, [MoViNets](https://arxiv.org/abs/2103.11511) are video classification models used for streaming video or online inference in tasks, such as action recognition. Consider using MoViNets to classify your video data for action recognition.\n",
        "\n",
        "A 2D frame based classifier is efficient and simple to run over whole videos, or streaming one frame at a time. Because they can't take temporal context into account they have limited accuracy and may give inconsistent outputs from frame to frame.\n",
        "\n",
        "A simple 3D CNN uses bidirectional temporal context which can increase accuracy and temporal consistency. These networks may require more resources and because they look into the future they can't be used for streaming data.\n",
        "\n",
        "![Standard convolution](https://www.tensorflow.org/images/tutorials/video/standard_convolution.png)\n",
        "\n",
        "The MoViNet architecture uses 3D convolutions that are \"causal\" along the time axis (like `layers.Conv1D` with `padding=\"causal\"`). This gives some of the advantages of both approaches, mainly it allow for efficient streaming.\n",
        "\n",
        "![Causal convolution](https://www.tensorflow.org/images/tutorials/video/causal_convolution.png)\n",
        "\n",
        "Causal convolution ensures that the output at time *t* is computed using only inputs up to time *t*. To demonstrate how this can make streaming more efficient, start with a simpler example you may be familiar with: an RNN. The RNN passes state forward through time:\n",
        "\n",
        "![RNN model](https://www.tensorflow.org/images/tutorials/video/rnn_comparison.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "dMvDkgfFZC6a"
      },
      "outputs": [],
      "source": [
        "gru = layers.GRU(units=4, return_sequences=True, return_state=True)\n",
        "\n",
        "inputs = tf.random.normal(shape=[1, 10, 8]) # (batch, sequence, channels)\n",
        "\n",
        "result, state = gru(inputs) # Run it all at once"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7xyb5C4bTs7"
      },
      "source": [
        "By setting the RNN's `return_sequences=True` argument you ask it to return the state at the end of the computation. This allows you to pause and then continue where you left off, to get exactly the same result:\n",
        "\n",
        "![States passing in RNNs](https://www.tensorflow.org/images/tutorials/video/rnn_state_passing.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "bI8FOPRRXXPa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d810a277-6852-4fc4-819e-aee02ea77ac9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "first_half, state = gru(inputs[:, :5, :])   # run the first half, and capture the state\n",
        "second_half, _ = gru(inputs[:,5:, :], initial_state=state)  # Use the state to continue where you left off.\n",
        "\n",
        "print(np.allclose(result[:, :5,:], first_half))\n",
        "print(np.allclose(result[:, 5:,:], second_half))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KM3MArumY_Qk"
      },
      "source": [
        "Causal convolutions can be used the same way, if handled with care. This technique was used in the [Fast Wavenet Generation Algorithm](https://arxiv.org/abs/1611.09482) by Le Paine et al. In the [MoVinet paper](https://arxiv.org/abs/2103.11511), the `state` is referred to as the \"Stream Buffer\".\n",
        "\n",
        "![States passed in causal convolution](https://www.tensorflow.org/images/tutorials/video/causal_conv_states.png)\n",
        "\n",
        "By passing this little bit of state forward, you can avoid recalculating the whole receptive field that shown above. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UsxiPs8yA2e"
      },
      "source": [
        "## Download a pre-trained MoViNet model\n",
        "\n",
        "In this section, you will:\n",
        "\n",
        "1. You can create a MoViNet model using the open source code provided in [`official/projects/movinet`](https://github.com/tensorflow/models/tree/master/official/projects/movinet) from TensorFlow models.\n",
        "2. Load the pretrained weights. \n",
        "3. Freeze the convolutional base, or all other layers except the final classifier head, to speed up fine-tuning.\n",
        "\n",
        "To build the model, you can start with the `a0` configuration because it is the fastest to train when benchmarked against other models. Check out the [available MoViNet models on TensorFlow Model Garden](https://github.com/tensorflow/models/blob/master/official/projects/movinet/configs/movinet.py) to find what might work for your use case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "rhSCM6cee05F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fd752cf-a30a-4980-d916-0679613442fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "movinet_a0_base/\n",
            "movinet_a0_base/checkpoint\n",
            "movinet_a0_base/ckpt-1.data-00000-of-00001\n",
            "movinet_a0_base/ckpt-1.index\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7f05e31ba5e0>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "model_id = 'a0'\n",
        "resolution = 224\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "backbone = movinet.Movinet(model_id=model_id)\n",
        "backbone.trainable = False\n",
        "\n",
        "# Set num_classes=600 to load the pre-trained weights from the original model\n",
        "model = movinet_model.MovinetClassifier(backbone=backbone, num_classes=600)\n",
        "model.build([None, None, None, None, 3])\n",
        "\n",
        "# Load pre-trained weights\n",
        "!wget https://storage.googleapis.com/tf_model_garden/vision/movinet/movinet_a0_base.tar.gz -O movinet_a0_base.tar.gz -q\n",
        "!tar -xvf movinet_a0_base.tar.gz\n",
        "\n",
        "checkpoint_dir = f'movinet_{model_id}_base'\n",
        "checkpoint_path = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "checkpoint = tf.train.Checkpoint(model=model)\n",
        "status = checkpoint.restore(checkpoint_path)\n",
        "status.assert_existing_objects_matched()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BW23HVNtCXff"
      },
      "source": [
        "To build a classifier, create a function that takes the backbone and the number of classes in a dataset. The `build_classifier` function will take the backbone and the number of classes in a dataset to build the classifier. In this case, the new classifier will take a `num_classes` outputs (10 classes for this subset of UCF101)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "6cfAelbU5Gi3"
      },
      "outputs": [],
      "source": [
        "def build_classifier(batch_size, num_frames, resolution, backbone, num_classes):\n",
        "  \"\"\"Builds a classifier on top of a backbone model.\"\"\"\n",
        "  model = movinet_model.MovinetClassifier(\n",
        "      backbone=backbone,\n",
        "      num_classes=num_classes)\n",
        "  model.build([batch_size, num_frames, resolution, resolution, 3])\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "9HWSk-u7oPUZ"
      },
      "outputs": [],
      "source": [
        "model = build_classifier(batch_size, num_frames, resolution, backbone, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhbX7qdTN8lc"
      },
      "source": [
        "For this tutorial, choose the `tf.keras.optimizers.Adam` optimizer and the `tf.keras.losses.SparseCategoricalCrossentropy` loss function. Use the metrics argument to the view the accuracy of the model performance at every step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "dVqBLrn1tBsd"
      },
      "outputs": [],
      "source": [
        "num_epochs = 2\n",
        "\n",
        "loss_obj = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
        "\n",
        "model.compile(loss=loss_obj, optimizer=optimizer, metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VflEr_t6CuQu"
      },
      "source": [
        "Train the model. After two epochs, observe a low loss with high accuracy for both the training and test sets. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZeiYzI0tqQG",
        "outputId": "e32c11f7-7e83-4ddf-85a0-e46928cf404c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "200/200 [==============================] - 400s 2s/step - loss: 0.2229 - accuracy: 0.9225 - val_loss: 0.1217 - val_accuracy: 0.9575\n",
            "Epoch 2/2\n",
            "200/200 [==============================] - 288s 1s/step - loss: 0.0964 - accuracy: 0.9669 - val_loss: 0.1487 - val_accuracy: 0.9350\n"
          ]
        }
      ],
      "source": [
        "results = model.fit(train_ds,\n",
        "                    validation_data=test_ds,\n",
        "                    epochs=num_epochs,\n",
        "                    validation_freq=1,\n",
        "                    verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkLl2zF8G9W0"
      },
      "source": [
        "## Evaluate the model\n",
        "\n",
        "The model achieved high accuracy on the training dataset. Next, use Keras `Model.evaluate` to evaluate it on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "NqgbzOiKuxxT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b886f186-55b5-49b5-9c29-195a17865922"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50/50 [==============================] - 55s 1s/step - loss: 0.1638 - accuracy: 0.9400\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'loss': 0.16378231346607208, 'accuracy': 0.9399999976158142}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "model.evaluate(test_ds, return_dict=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('moivnet_TrLn_vio_vdo_model')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iE7UAhHCjhfQ",
        "outputId": "29a42e50-73c3-4217-ddec-fabe710a0c73"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x7f05e2b80ee0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x7f05e2a01370>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x7f05e29013d0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x7f05e28a4b20>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x7f05e31e34c0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x7f05e2e953d0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x7f05e2db64f0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x7f05e2cf8280>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x7f05e2bdf430>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x7f05e2ab7a60>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x7f05d0580250>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x7f05d0507d30>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x7f05d04405b0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x7f05d03708e0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.modeling.layers.nn_layers.SpatialAveragePool3D object at 0x7f05d0318dc0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x7f05e2898700>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.modeling.layers.nn_layers.TemporalSoftmaxPool object at 0x7f056357ef10>, because it is not built.\n",
            "WARNING:absl:Found untraced functions such as init_states, head_layer_call_fn, head_layer_call_and_return_conditional_losses, classifier_layer_call_fn, classifier_layer_call_and_return_conditional_losses while saving (showing 5 of 691). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "saved_model_dir='/content/drive/MyDrive/movinet/moivnet_TrLn_vio_vdo_model'\n",
        "# Convert the model\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir) # path to the SavedModel directory\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the model.\n",
        "with open('model.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_zgKlKWNi9P6",
        "outputId": "7c59e0c9-b62e-4cb0-fea5-0d3dc06a4c18"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ConverterError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mConverterError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-6e92f518fd01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Convert the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFLiteConverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaved_model_dir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# path to the SavedModel directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtflite_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Save the model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    931\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_and_export_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m     \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36m_convert_and_export_metrics\u001b[0;34m(self, convert_func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    909\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_conversion_params_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    912\u001b[0m     \u001b[0melapsed_time_ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1214\u001b[0m           graph_def)\n\u001b[1;32m   1215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_from_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36m_convert_from_saved_model\u001b[0;34m(self, graph_def)\u001b[0m\n\u001b[1;32m   1097\u001b[0m     \u001b[0mconverter_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquant_mode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverter_flags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_convert_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconverter_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m     return self._optimize_tflite_model(\n\u001b[1;32m   1101\u001b[0m         result, quant_mode, quant_io=self.experimental_new_quantizer)\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/convert_phase.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m           \u001b[0mreport_error_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mconverter_error\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# Re-throws the exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0mreport_error_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/convert_phase.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mConverterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconverter_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconverter_error\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/convert.py\u001b[0m in \u001b[0;36mconvert_saved_model\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    806\u001b[0m   \u001b[0mmodel_flags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model_flags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m   \u001b[0mconversion_flags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_conversion_flags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 808\u001b[0;31m   data = convert(\n\u001b[0m\u001b[1;32m    809\u001b[0m       \u001b[0mmodel_flags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m       \u001b[0mconversion_flags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/convert.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(model_flags_str, conversion_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\u001b[0m\n\u001b[1;32m    308\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0merror_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_metrics_wrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve_collected_errors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0mconverter_error\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mconverter_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m   return _run_deprecated_conversion_binary(model_flags_str,\n",
            "\u001b[0;31mConverterError\u001b[0m: <unknown>:0: error: loc(callsite(callsite(fused[\"AvgPool3D:\", \"movinet_classifier_1/movinet/block0_layer0/bneck/skip/skip_pool/AvgPool3D@__inference__wrapped_model_2242608\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_2269447\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): 'tf.AvgPool3D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"AvgPool3D:\", \"movinet_classifier_1/movinet/block0_layer0/bneck/skip/skip_pool/AvgPool3D@__inference__wrapped_model_2242608\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_2269447\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"AvgPool3D:\", \"movinet_classifier_1/movinet/block1_layer0/bneck/skip/skip_pool/AvgPool3D@__inference__wrapped_model_2242608\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_2269447\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): 'tf.AvgPool3D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"AvgPool3D:\", \"movinet_classifier_1/movinet/block1_layer0/bneck/skip/skip_pool/AvgPool3D@__inference__wrapped_model_2242608\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_2269447\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"AvgPool3D:\", \"movinet_classifier_1/movinet/block2_layer0/bneck/skip/skip_pool/AvgPool3D@__inference__wrapped_model_2242608\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_2269447\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): 'tf.AvgPool3D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"AvgPool3D:\", \"movinet_classifier_1/movinet/block2_layer0/bneck/skip/skip_pool/AvgPool3D@__inference__wrapped_model_2242608\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_2269447\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"AvgPool3D:\", \"movinet_classifier_1/movinet/block4_layer0/bneck/skip/skip_pool/AvgPool3D@__inference__wrapped_model_2242608\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_2269447\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): 'tf.AvgPool3D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"AvgPool3D:\", \"movinet_classifier_1/movinet/block4_layer0/bneck/skip/skip_pool/AvgPool3D@__inference__wrapped_model_2242608\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_2269447\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: failed while converting: 'main': \nSome ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select \nTF Select ops: AvgPool3D\nDetails:\n\ttf.AvgPool3D(tensor<?x?x?x?x32xf32>) -> (tensor<?x?x?x?x32xf32>) : {data_format = \"NDHWC\", device = \"\", ksize = [1, 1, 3, 3, 1], padding = \"SAME\", strides = [1, 1, 2, 2, 1]}\n\ttf.AvgPool3D(tensor<?x?x?x?x56xf32>) -> (tensor<?x?x?x?x56xf32>) : {data_format = \"NDHWC\", device = \"\", ksize = [1, 1, 3, 3, 1], padding = \"SAME\", strides = [1, 1, 2, 2, 1]}\n\ttf.AvgPool3D(tensor<?x?x?x?x8xf32>) -> (tensor<?x?x?x?x8xf32>) : {data_format = \"NDHWC\", device = \"\", ksize = [1, 1, 3, 3, 1], padding = \"SAME\", strides = [1, 1, 2, 2, 1]}\n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/file.zip /content/moivnet_trained_model_on_ucf101subset "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMt3ZnQ0koas",
        "outputId": "2e9c94e4-3f8d-4eb9-d565-4fe815ef92c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/moivnet_trained_model_on_ucf101subset/ (stored 0%)\n",
            "  adding: content/moivnet_trained_model_on_ucf101subset/variables/ (stored 0%)\n",
            "  adding: content/moivnet_trained_model_on_ucf101subset/variables/variables.index (deflated 75%)\n",
            "  adding: content/moivnet_trained_model_on_ucf101subset/variables/variables.data-00000-of-00001 (deflated 9%)\n",
            "  adding: content/moivnet_trained_model_on_ucf101subset/assets/ (stored 0%)\n",
            "  adding: content/moivnet_trained_model_on_ucf101subset/saved_model.pb (deflated 92%)\n",
            "  adding: content/moivnet_trained_model_on_ucf101subset/keras_metadata.pb (deflated 96%)\n",
            "  adding: content/moivnet_trained_model_on_ucf101subset/fingerprint.pb (stored 0%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('/content/file.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "9WUVEiIzk53v",
        "outputId": "eb60f232-56b2-4d31-ada8-282e761971e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_8f64d9b8-4df5-474f-af56-44a01a2f5022\", \"file.zip\", 15367325)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('moivnet_tr_ln_model_Viovideo.h5')"
      ],
      "metadata": {
        "id": "f7AgT0WWjxyc"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_weights(\"ckpt\")\n"
      ],
      "metadata": {
        "id": "tIfxomJ1Urv-"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_weights(\"saved_model_trained_weights.h5\")\n"
      ],
      "metadata": {
        "id": "qiRVR1xhVKqX"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkFst2gsHBwD"
      },
      "source": [
        "To visualize model performance further, use a [confusion matrix](https://www.tensorflow.org/api_docs/python/tf/math/confusion_matrix). The confusion matrix allows you to assess the performance of the classification model beyond accuracy. To build the confusion matrix for this multi-class classification problem, get the actual values in the test set and the predicted values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "hssSdW9XHF_j"
      },
      "outputs": [],
      "source": [
        "def get_actual_predicted_labels(dataset):\n",
        "  \"\"\"\n",
        "    Create a list of actual ground truth values and the predictions from the model.\n",
        "\n",
        "    Args:\n",
        "      dataset: An iterable data structure, such as a TensorFlow Dataset, with features and labels.\n",
        "\n",
        "    Return:\n",
        "      Ground truth and predicted values for a particular dataset.\n",
        "  \"\"\"\n",
        "  actual = [labels for _, labels in dataset.unbatch()]\n",
        "  predicted = model.predict(dataset)\n",
        "\n",
        "  actual = tf.stack(actual, axis=0)\n",
        "  predicted = tf.concat(predicted, axis=0)\n",
        "  predicted = tf.argmax(predicted, axis=1)\n",
        "\n",
        "  return actual, predicted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "2TmTue6THGWO"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(actual, predicted, labels, ds_type):\n",
        "  cm = tf.math.confusion_matrix(actual, predicted)\n",
        "  ax = sns.heatmap(cm, annot=True, fmt='g')\n",
        "  sns.set(rc={'figure.figsize':(12, 12)})\n",
        "  sns.set(font_scale=1.4)\n",
        "  ax.set_title('Confusion matrix of action recognition for ' + ds_type)\n",
        "  ax.set_xlabel('Predicted Action')\n",
        "  ax.set_ylabel('Actual Action')\n",
        "  plt.xticks(rotation=90)\n",
        "  plt.yticks(rotation=0)\n",
        "  ax.xaxis.set_ticklabels(labels)\n",
        "  ax.yaxis.set_ticklabels(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "4RK1A1C1HH6V"
      },
      "outputs": [],
      "source": [
        "fg = FrameGenerator(subset_paths['train'], num_frames, training = True)\n",
        "label_names = list(fg.class_ids_for_name.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "r4AFi2e5HKEO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "a4824845-074b-4ad2-b783-74a94a4ebc78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50/50 [==============================] - 63s 1s/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAFPCAYAAAAY81r6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd7wcZdn/8c83oYQEQg2BiCSAlAfpBOmIgkpRQR4EIiggGkSliQUUJIgoKCDNHxBKEhCIKFUUaRLABykJJZDQIaFISSihJJScc/3+uGfJZrPnnD1ld+Zsvu+89pWz987OXDM7O9fcZWYVEZiZmRVNn7wDMDMzq8YJyszMCskJyszMCskJyszMCskJyszMCskJyszMCskJCpDUX9J5kl6WFJLOqMMypkka29PzbQbZNh+Vw3L7SPqNpOcltUq6ttExVIlprKRpecfRrCRNkDShxmkPyPbNYXUNqu3l7y3pMUkfSnorjxjyVpgEJWmQpJMlTZH0nqTZkiZnZUPqvPgfASOBC4BvApfWeXlNSdI3JB2Rdxyd8A3gGOB6YH/gD41YqKR1JY3K68Bn82Qnp6MkbZ93LOUkrQb8CXgBOJh0fKrn8hqyT3b2GKEiXKgraRPgRmBp4ArgPqAV2ADYG3gjItaq4/JvBZaPiI3ruIzFgdaI+Khey8ibpBuA9SJiWCff1w+YGxFz6xJY28u9ENgjIpZr8HL3BP4CfC4iJlS8tijQJyI+aGRMCwtJiwFExIfZ8xWAGcAJETGqYtq+wKLAB9HgA6Wk/UgnyhtFxMMNWF6b+2QPL6dTx4hF6hVIrSQtDZSaVjaNiCkVr/8c+Fmdw1gReKOeC/ABZ37Zl3+RiPggIt7PKYwVgbdzWnZVRTqBkSSgX0TMyTuWnlJKTDVO2wK01DGc9qyY/T+rp2YoqX9EzO6p+TVEROT6AH4KBLBfJ97zv8BEYA7wOqnWtWrFNGOB94FPkBLgu6QzpVOBvtk022fLrnwMAw4o/V0x39J7ti8r+xRwJfAy8AHwX+AqYOWyaaYBYyvmtQIwGngli/VR4LsV0wzLlnc08F3gmWwZ9wOb1bCtSvF+AzgeeCnbFlcDywKLAacDr2bl44AlKuZxAHBrFucHwFOkprE+ZdNMqLYtq6zDD7P3zy1tw+y1UdnfAm7PPtfy7dcXuCfbtst1sM79gd8Dz5fFe3Qp3rJ4Kh/btzPPbYE/A9Ozeb5MahJeIBZgZeB84MVs2mnZtEuV7VeVjwPK9ttpFfPrC/wCeDqb3/PA76p8TtOAfwLbkFoh3geeBb5V4/cqgPNIrRaPAB+WxbV0tp88n5U/CxxH9l0qm4eAHwAPkb6fM4FbgG27sD59gFHZZz472y8+TcV3qWybfjaLcQbwHnANMKhinhOACR3sB2Mr5lt5DOiR4087n8O0KjGNKnv9e6Rjxfuk7+T5VOyH2Xo+DmyUbbf3qDj+VNl+VffJbJrNgH+QEuYc4N+k2lb5fJbM1u+57HOdkcWxXUfHiLYeudeggK+SNvSVtUxcVvWdRDpIDgIOA7aRtHFEzCybvA/pC3sf8GNgR+Ao0kH+XOAxUp/Tb0k70EnZ+2bUGnzWJHMTsATwR9KBa2VgJ2BI9rza+/qRdpx1svc9A+wOjJa0fEScXPGWvUk7wPmkD/anwNWSVo/azrp/StrOJ5MS6qGks8PZwGDgBGBz4Fukg/Avy977A9K2+kc2jx2A35AOWkdn05yUPV8FOLKNGL4JDCAl5Xeosm0iIiQdQDpAXgB8OXvp6Cy+XSOizdpudtZ/LfAF4GLSfrID6TMeRvpyz8hiOaoi3sfami/w9Wz9RgOvkZqfvwOsJ2mrUiaWtBJpfyudfEwh7QdfA5YH7gTOIu2zvylb5t3tLPt84CDSScXpwHDgJ9mydy0tO7Ma8FfgItLJxreBsZImRUXrRBu2A/YEziEd/B6XtARpXx1GSmDTgM+QksfQbDuUjM6e30Q6SAvYKpvvXZ1cn9+S9tsbSN/jDbL5Lt5G7GeQWkJOyGI9IluPvduYfgZwCOlYcE0WD6TvYlU9fPxpyxHAXsAI0r45E5icLf9Y4ETgX6TtuAbp+7m5pM1j/paaZUjb6ypSEm1roEW7+6Skz2bzeQj4FfAR6ftzs6QvxLwmwXOzuP9I2u+XJX1nN8yWUcsxYn61nFnV80HaoR6qcdpFSV+aqZSdbTGvlnBqxRlMAL+smMcDwMSKskfJzqqqnFVUnj2VllU6+98we75nDWdF5Wd9h2Xv27+srC+ppvI+qU8M5p3lzQSWLZv2q1n5lztYbineqcBiZeWXk/r5biLri8zK7wZerJhH/yrzHU1K6ouXld1Axdl/xTq8S1mtqOz1+c4Qs7JvZ+UHkc4CPwQuqGEfKW2X4yvKx2Tl63UUbxvzrbYNvpHNc5uK/a4F2LzK9KU+3z3L96GKacaWx0Q6KAcwpmK6UZWfP/POvLcrKxuU7U+ntrd+ZZ9DK6nfo7z856QTmXUqyn+RvWftin3tj+2se03rQzpp+og0gKV8/zyeslpOxXf11oppTyfV1JcuK5tA2XeddCKxwP5XMd9h2fO6HH/a+Cx+TMXxJ/ssP8jWs2+VOH9YsZ4BHFbj/l11nySdYDxeZdsuRkpCd5eVvQmc08Fyav7ORUQhRvENJJ1N12I4acc9N8raxSNl8EnArlXec0HF87uA1TsfZptKfRhfkjSgE+/blXQG96dSQaQ27zNIZ4g7Vkx/VUS8Wfa8dDZa67pcGvO3v99L2vnGRLbnlJUPyQZ1lOKaDanfSNKyWcfyHaTa0Do1Lh/g2oioWqOsFBEXA38jHWQuJzVN/qiGt+5KOsieWVF+WtnrnVa2DSRpYLYNSrWeTbPX+pBqSjdGxL1V5hGVZTUoxXt6RfkfSImwcn2ejIg7y5Y5A3iC2veTuyPioYqyvUhNOjMlrVB6kA5akA7QkA5ykJLIfMrWvdb12YHUR35uxXY7u53YL6qY9i7SSd/Qdt7TGXkff3YkJYYzs2NFyaWkJvrK5c8lnUh2x4bA2qTv4PJln/1AUtPt5pL6Z9POyp5/opvL/FgREtTbpLb5WpR2tCeqvPYY6Uy93EdVDohvkqqePSIiniN92b5D+gLfKulwSct38NahwNMVOxrMq14Pqyh/vmK5pWRV67o8X/G81Pn6QpVykZoHAJC0jaQ7SWfRbzB/Yl26xuVDO00nbfgu6SD1P8C3I6KWE5mhwKsRUdmc8QQpcQ3rZAwASPqkpPGk7TOLtA2ey14ubYNBpC/uo11ZRhuGks5snywvjIhZpCbSYRXTV37O0Ll9vtpntBapyXRGxeOe7PVSh/4apG0/c4E5zFPr+pS+609XTPcGaX2qqVz3zn5HOpL38afq8rNjyFNVlv/f6P4ApNLo6YtY8PM/nJRDSse6n5D6CJ+XNFHSryWt3Z2FF6EP6jFgE0mLRSdG2NSotRvvbetst+8CE0YcJeliUvPSF0ln68dK+mxETO1GDOXaGk2kbr6/3flKWp10pvwkqd34eVKT0SbAKXTuJKezo8G2JQ14AFif1A/ScNmIw5tJCei3pH32Peb1MRThRK+ku/tJtc+oD6nP47dtvOfZGuddb91d957WneNPT+iJ0ZelfftoUi2xmhkAEfEXSXcBu5GOg4cBP5V0QERc3pWFFyFBXU/qRP06cFkH007P/l+bdMAotw6pDb6nlM6+lqkor9pcEKkDegrwW0kbkD7MI0m1gGqmAxtL6ltRiyo1mU3rStB18FVSk+NXIqK0/UsXElbqShNWVZIGkzpd7yB9AX4r6caIeKqDt04HviBp6eysvGQt0pdtWhfCWZ/0uRwQEePKYlyzYroZpBaB9TqYX2e203TSAXYt0sCR0rIHkgbj3NCJeXXVM8BSEXFrDdPtJGlQ1rRYTa3rU9rXPkWqHZSmW54ebAGh858FNOb409HyP66BZk3LawIPdmPebW2HUo36nRo+fyKiNKrwfEnLkGrZJ5CaCNtbTlVFOPM7n9S/cJqk/6l8UdJSkkqj6yaS2loPzkbBlabZltQ+3JNf1tIHs13ZcvpScUV31h9RmegfI529VCa3cjeQzsi/UTavPqRqc6kjtAhKyfPjs9Csf+qHVaZ9D1gmG0nXXecD/YADSSOt3gHGZduoPTeQ9uvDKspL/Vd/70IsC2yDzI/Ln0REK2k02M6SNq+cSdl2eS/7v5YDbSneyqvvDyfV5huRoP4MbCZpl8oXsu9nqb/yr9n/o6pMV1r3WtfnNlIfyiEV01Xb77qjdF1QLZ9FI48/1dxCGix0WMX3YF9S31h3lt/WPjmJ1Mz6I0kLdMVIGpT93ze7pvVjWTP7c8x/HOzUMSL3GlREvCVpd9IQ5gckXU66xqeVdCY6gtTv8YuI+EjST4BLgLsk/Yl5wzxfIjU59VRcUyTdQzpzXy6LYR8W3GafB/4o6a+ktmGRhrUuRfpit+UCUrK7SNLGpGaS3Umdw8dExOs9tS7ddBPpS3GDpPNJtalvUr35YiJp3c+QdC/pzhnjO7tASfuTmgm+l/XxIWkkcB2pnbu9z/kG0hf5BElDSaOmPk+6duX8iOhK/9DjpLP40yStQtoXdiYNl610DKm/ZkK2vaaSDh57kAZQTCOd6bYAx2RnmXOAe0vrWi4iJku6CDgoOwD8i9S8+m1S8+I/urA+nfV74CvAdZLGkQ5aS5C+n18n1TCnRcQEpftNfl/SGqS7wwBsSRom/Zta1yciXpV0JnCUpL9l89oQ2IU0orVHausRMUfSFGAfSU+Srmt6ro1BLg07/rQR60xJJ5KGmd+sdO/I1UlJ+2Hgwm7Mvs19UtJBpM9mataV8SLp0onPko53nyMd716SdFUWy9vA1qTLbc4pW07njhG1Dver94PU0XoK6Qs9O9tAk0lj5wdXTFu6UO590sFiPDC0YpqxwPtVljOKiovDqDLMPCtfnXSwK10QdxJpJE35MPPVSDvGU8wbRHAXsFvFvKZR/ULdC0hnZR+QmgjbvFC3SnxVh8dWTLN9Nt0+FeUHZOVbVNs+wEplZTuTduA5pEEVJ5EOwvMNSyX1F40jfclbS9u5vXWoXA/SQf8t4KYq043JPov1OljnAaQLBl8kJdenSYmj8qLSzgwzX5v0JZ2VfcaXkxLPAp9Btg5jyj7X50g1wiXLpjmQeRcsB7VdqPtMtj4v0M6FulVin0CV/buNz+G8drbpr0lNSx+QksR/SNcp9Subrg+paXtK2XQ3A1t3YX36kq67eZn03bqN1JQ2kzSSrqN9efsq++gC24J0rc692b718RB2Or5Qt0eOP21s7wWGmZe99r2y7fsqaaTe8lU+88dr2bc72iez1zYg3QppRrbc6aQa807Z64tln+GDpO/ve6Tj6lGkO8a0e4xo61GIe/GZmdUiO7t/Ezg2Ik7qaHrr3YrQB2VmtoDsDhaVSn1XExoYiuUk9z4oM7M27K1026t/kO5Csg2pT/rmiPi/PAOzxnCCMrOimkzqD/kp6QLoV0l3CDk2z6CscdwHZWZmheQ+KDMzKyQ38TXQRzOfdXXV5rPEkG3zDsEKaO6HL3X7YvfOHG8WXWH1vG4H1S4nKDOzZtSa148B9xwnKDOzZhR536u2+5ygzMyaUasTlJmZFVC4BmVmZoXkGpSZmRVSy0d5R9BtTlBmZs3ITXxmZlZIbuIzM7Mi8iAJMzMrJtegzMyskFyDMjOzQvIoPjMzKyQ38ZmZWSG5ic/MzArJNSgzMyuiCP/chpmZFVHL3Lwj6DYnKDOzZuQ+KDMzKyT/oq6ZmRWSa1BmZlZIHsVnZmaF5BqUmZkV0lyP4jMzswLydVBmZlZM7oMyM7NCch+UmZkVkmtQZmZWSK5BmZlZIflefGZmVkhu4jMzs0JygjIzs0JyH5SZmRWSa1BmZlZIHiRhZmaF5CY+MzMrJDfxmZlZITlBmZlZIUXkHUG3OUGZmTUj16DMzKyQPIrPzMwKyTUoMzMrJPdBmZlZIbkGZWZmheQEZWZmRRQtLXmH0G1OUGZmzcg1KDMzK6QmuBdfn7wDMDOzOmiN2h8dkHSxpNckPVpWNkrSS5Ieyh67lL12jKSnJT0h6UtdXQXXoMzMmlHPNvGNBc4BLqko/0NEnFpeIGldYB/g08AQ4FZJa0VEpzvF6pagJAVwekQclT3/MbBkRIzqwryeBXaOiCfKys4AXgZmAbMjonLDlb9/LHBDRPy1s8u2zjv2N6dz5//dx3LLLsO1fzoPgMefepYTf382s+e8z5CVV+SU43/KkgMGAPDE08/xq9+dxbvvzaZPnz6Mv/BMFl98sTxXwXLQp08f7r3nRv770ivs9rX98w6n9+vBBBURd0oaVuPkuwHjI+ID4DlJTwOfAf7T2eXWs4nvA2APSSv0wLzGkzIyAJL6AHuSNsJ57SUna7zdd/kC553+6/nKjj/5DI445ECuufRcdthuK8ZcdhUAc+e2cPSvfsdxPzmU6y47nzHnnMIii/TNI2zL2WGHfofHH38q7zCaR0tL7Y+u+6GkyVkT4LJZ2SeAF8qmeTEr67R6Jqi5wGjgyMoXJA2T9K9sxW6TtGpWPlbSWZLulvSspD2zt1wB7F02i+2A6RExPWsH/XH2/o0k3ZPN95qyDVa+7F9Kul/So5JGS1JWPkHSKZLuk/SkpG2z8r6STs2mnyzp0Kx8U0l3SJok6SZJK/fcpuvdhm+0PksPXGq+sukvvMTwjdYHYMvNNuGWO/4NwN33TWKtNVZjnTVXB2CZpQfSt68T1MLmE59YmV123oGLL74i71CaRyf6oCSNlDSx7DGyhiWcC6wBbERqzTqtp1eh3oMk/gjsK2npivKzgXERsQFwGXBW2WsrA9sAXwZOBoiIR4BWSRtm0+xDSlqVLgF+ls33EeD4KtOcExGbRcR6wBLZckoWiYjPAEeUvXckMAzYqBSvpEWzddgzIjYFLgZOandLLOTWWG0o/7or1fBvvv0uXnl1JpASlyRGHvkLvn7gD7n4sr/kGabl5PTTTuDoY35NaxMMjS6MaK35ERGjI2J42WN0h7OPeDUiWiKiFbiA1IwH8BLwybJJV8nKOq2uCSoi3iYljcMqXtoSuDz7+1JSQiq5NiJaI2IqMLis/ApgH0mLALsD8x3JsiS4TETckRWNI9W0Kn1O0r2SHgE+T+rIK7k6+38SKSkB7AicHxFzs3V6A1gbWA+4RdJDwLGkD2EB5WcmF16y8J4dnvjzIxl/9Q3s9e1DeW/2HBZdNHV/zm1p4cHJUzjl+J9yybmnctsdd3PPxAdzjtYaadddduS112bywIOP5B1Kc+nBUXzVVLQafQ0ojfC7nnSsXlzSasCawH1dWUYjRvGdATwAjKlx+g/K/lbZ3+OBm4E7gMkR8WpnA5HUD/h/wPCIeEHSKKBflWW30P62ETAlIrbsaJnZmchogI9mPtv7797YRasP/SQXnPEbAKY9/yJ33p3218ErrsCmG67HssukSva2W27G1CeeYYvhG+cWqzXWVlsN5ytf/iI77/R5+vVbnIEDl2Lc2LPY/4DK81rrjOjB2qikK4DtgRUkvUhqYdpe0kZAANOAgwEiYoqkK4GppK6eH3RlBB804DqorMZxJXBQWfHdzBv0sC9wVw3zeQaYSWr2W6AqEhGzgDdLfUfAN0nJrFwpGc2UtCRpoEVHbgEOzmpuSFoOeAIYJGnLrGxRSZ9uZx4LvdfffAuA1tZWzh83nr12T5dMbP2ZTXnq2WnMef995s5tYeJDj7DGaqvmGao12C+OPZlhqw/nU2ttwb77fZ/bb/8/J6ee0IM1qIgYERErR8SiEbFKRFwUEd+MiPUjYoOI+GpEvFw2/UkRsUZErB0RN3Z1FRp1HdRpwA/Lnh8KjJH0E2AGcGCN87mClKCubuP1/YHzJPUHnq2cb0S8JekCUlX0FeD+GpZ5IbAWMFnSR8AFEXFONoDjrKxpcRFSTXFKjevR1H5y/Mnc/+Bk3nrrbXbYfT++f9A3mT1nDuOvvgGAHT+7FV/b9YsALD1wKb61zx7sc9DhSGLbLTfjs1t9pr3Zm1ktmuBefIom+M2Q3mJhbuKz6pYYsm3HE9lCZ+6HL6njqdr33qgRNR9vBoy6otvLqwffScLMrBl1cfBDkThBmZk1oya4WawTlJlZM3INyszMiijm9v5BEk5QZmbNyDUoMzMrJPdBmZlZIbkGZWZmRRROUGZmVkhOUGZmVkgexWdmZoXkGpSZmRVRM9xn1QnKzKwZuQZlZmaF5ARlZmZF5GHmZmZWTHOdoMzMrIBcgzIzs2JaWBKUpL7A4PLpI+L5egVlZmbd1PvvFdtxgpJ0KHA88CrzVjmADeoYl5mZdcPC0sR3OLB2RLxe72DMzKxnxEIySOIFYFa9AzEzsx60MDTxAc8CEyT9HfigVBgRp9ctKjMz65Ym+L3CmhLU89ljsexhZmZFtzAkqIg4AUDSktnzd+sdlJmZdU8z1KD6dDSBpPUkPQhMAaZImiTp0/UPzczMuqy1E4+CqqWJbzTwo4i4HUDS9sAFwFZ1jMvMzLqhdW7eEXRfLQlqQCk5AUTEBEkD6hiTmZl1UzM08dU0ik/SccCl2fP9SCP7zMysqEJ5R9BtHfZBAd8GBgFXZ49BWZmZmRVUtNb+KKpaRvG9CRzWgFjMzKyHRGvvr0G1maAknRERR0j6G+nee/OJiK/WNTIzM+uyIteMatVeDarU53RqIwIxM7Oe09rSxDWoiJiU/blRRJxZ/pqkw4E76hmYmZl1XTM08dUySGL/KmUH9HAcZmbWgyJqfxRVe31QI4BvAKtJur7spaWAN+odmJmZdV0z1KDa64O6G3gZWAE4raz8HWByPYMyM7PuaeoEFRHTgemS9gX+GxHvA0haAlgFmNaQCM3MrNOaYZBELX1QVzL/7QRbgL/UJxwzM+sJEar5UVS13OpokYj4sPQkIj6U5N+FMjMrsGa4DqqWGtQMSR9flCtpN2Bm/UIyM7Puag3V/CiqWmpQ3wMuk3QOIOAF4Jt1jcrMzLqlyE13tarlXnzPAFuU/6KupM2AZ+odnJmZdU1Tj+KrYlVghKR9gFnA8PqEZGZm3dUMo/jaTVCShgEjssdHwFBgeERMq3dgZmbWdUXuW6pVm4MkJP0H+Dspif1vRGwKvOPkZGZWfM0wzLy9UXyvkm5rNJj0I4VQ5Wc3zMyseHryXnySLpb0mqRHy8qWk3SLpKey/5fNyiXpLElPS5osaZOurkObCSoidgfWByYBoyQ9Bywr6TNdXZiZmTVGDw8zHwvsVFF2NHBbRKwJ3JY9B9gZWDN7jATO7eo6tHsdVETMiogxEfFFYHPgOOAPkl7o6gLNzKz+erKJLyLuZMGbhO8GjMv+HgfsXlZ+SST3AMtIWrkr61DzKL6IeA04BzhH0tCuLGxht/66e+cdghXMu7f/Lu8QrEm11H+Y+eCIeDn7+xVSdxDAJ0jXy5a8mJW9TCfVcieJBWQ3kjUzs4LqTA1K0khJE8seIzu3rAjqMEahM9dBmZlZL9GZYeYRMRoY3clFvCpp5Yh4OWvCey0rfwn4ZNl0q2RlndalGpSZmRVbdOLRRdcz7xfX9weuKyv/VjaabwtgVllTYKe094u6Z9NO7BFxWFcWaGZm9deTF+pKugLYHlhB0ovA8cDJwJWSDgKmA3tlk/8D2AV4GpgNHNjV5bbXxDexqzM1M7N8tfRggoqIEW28tEOVaQP4QU8st71f1B3X1mtmZlZsQXHvEFGrDgdJSBoE/AxYF+hXKo+Iz9cxLjMz64bWJrjvTy2DJC4DHgNWA04ApgH31zEmMzPrplZU86OoaklQy0fERcBHEXFHRHwbcO3JzKzAAtX8KKparoP6KPv/ZUm7Av8FlqtfSGZm1l2teQfQA2pJUL+WtDRwFHA2MBA4sq5RmZlZt7QUuGZUq1p+8v2G7M9ZwOfqG46ZmfWEhaIGJWkMVS7YzfqizMysgIrct1SrWpr4bij7ux/wNVI/lJmZFVT9b2Zef7U08V1V/jy75cW/6xaRmZl1W5GHj9eqK3czXxNYsacDMTOzntOSdwA9oJY+qHeYvw/qFdKdJczMrKBatRDUoCJiqUYEYmZmPacJ7nTU8Z0kJN1WS5mZmRVHayceRdXe70H1A/qTfv9jWfi4x20g6fflzcysoJp9FN/BwBHAEGAS8xLU28A5dY7LzMy6oalH8UXEmcCZkg6NiLMbGJOZmXVTS+/PTzXdzbxV0jKlJ5KWlfT9OsZkZmbd1Ax9ULUkqO9GxFulJxHxJvDd+oVkZmbdFZ14FFUtF+r2laTsd+aR1BdYrL5hmZlZdzT7IImSfwJ/lnR+9vzgrMzMzAqqyE13taolQf0MGAkckj2/BbigbhGZmVm3NUOC6rAPKiJaI+K8iNgzIvYEppJ+uNDMzAqqRbU/iqqmm8VK2hgYAewFPAdcXc+gzMyse5qhBtXenSTWIiWlEcBM4M+AIsK/qmtmVnBFHp1Xq/ZqUI8DdwFfjoinASQd2ZCozMysW5phFF97fVB7AC8Dt0u6QNIO0AT3zjAzWwg09YW6EXFtROwDrAPcTrov34qSzpX0xUYFaGZmndfSiUdR1TKK772IuDwivgKsAjyIf7DQzKzQWlX7o6hqudXRxyLizYgYHRE71CsgMzPrvmZo4qtpmLmZmfUuzT6Kz8zMeqnWJkhRTlBmZk2oyE13tXKCMjNrQkUenVcrJygzsyZU5NF5tXKCMjNrQu6DMjOzQur96ckJysysKXmQhJmZFZKb+MzMrJA8is/MzArJNSgzMyuk3p+enKDMzJqSB0mYmVkhRRPUoXpFgpJ0O3ByRNxUVnYEcDhwfkSc3M57RwHvRsSpdQ/UFrDSkMGccs4olh+0HBFw5aXXcOkF4z9+/cBD9uVnJxzBFuvsyFtvzMoxUqu3X158PXc+/CTLDRzA1SceAsBPzv0r0195HYB3Zr/PUv37ceUJBwPw5AuvcuIlN/DunA/pI3H5L7/D4ov2ikNWIcx1gmqYK4B9gJvKyvYB9o+IO/MJyWrRMncupxx/BlMfeYIBA/pz1a2XcPcd9/LMk8+x0pDBbL395rz0wst5h2kNsNvWGzJih834xT6S+5IAABBlSURBVIXXflz2+0P2/PjvU8ffzJL9FwdgbksrP7/gGk76zu6svepKvPXubBbp26mfr1vo9f701MkfLMzRX4FdJS0GIGkYMARYQ9I5pTJJ/5I0WdJtklatnImk70q6X9LDkq6S1D8rHyvpLEl3S3pW0p5l7/mZpEey95ycla0h6Z+SJkm6S9I6dd8CvdSM115n6iNPAPDee7N55slpDF55EADHnHgkv//V2RDN8FWyjmy69lAGDlii6msRwc33T2XnzdcD4D9TnmHNVQaz9qorAbDMkv3p26e3HK6KoZWo+VFUveITj4g3gPuAnbOifYArmf8k4WxgXERsAFwGnFVlVldHxGYRsSHwGHBQ2WsrA9sAXwZKiWhnYDdg8+w9v8umHQ0cGhGbAj8G/l+3V3Ih8IlPrsz/rL82D0+awud32o5XX57BE1OeyjssK4AHnnye5QcOYOjg5QGY/srrSPC90/7E3qNGM+bG/8s5wt7Hv6jbWKVmvuuy/w8C1i97fUtgj+zvS5mXTMqtJ+nXwDLAkszfZHhtRLQCUyUNzsp2BMZExGxIiVLSksBWwF+kj28XvHg3163p9R+wBGddfAq/Pe50WlrmcvDhB3LQXj/MOywriBvvfZSdstoTQEtrKw8+9QKXH/cd+i22KCNPvYR1h67M5uuunmOUvUtPD5KQNA14h3QN8NyIGC5pOeDPwDBgGrBXRLzZU8vsFTWozHXADpI2AfpHxKQuzGMs8MOIWB84AehX9toHZX+3d6P6PsBbEbFR2eN/2ppY0khJEyVNfGvOjC6E3Pstskhfzrr4FP521T+55e+3s+qwVVhl1SFcd/vl3DbxOgYPWZGrb/0TK6y4fN6hWg7mtrRy2wOPs9NnPv1x2YrLDmTTtVZl2aX6s8Tii7LN+mvy2PRXcoyy96lTDepz2TFvePb8aOC2iFgTuC173mN6TYKKiHeB24GLSbWpSneTalYA+wJ3VZlmKeBlSYtm03TkFuDAsr6q5SLibeA5SV/PyiRpw3biHh0RwyNi+DJLDKphkc3n12ccxzNPTmPseZcD8ORjz7D1p7/EDsN3Y4fhu/Hqf19jjx33Y+Zrr+ccqeXh3qnPstpKyzN4uYEfl2293ho89eJrzPngI+a2tDLpiemsPmSFHKPsfVqImh/dsBswLvt7HLB7twMv05ua+CAlpmuYl4jKHQqMkfQTYAZwYJVpjgPuzV6/l5Sw2hQR/5S0ETBR0ofAP4Cfk5LbuZKOBRYFxgMPd2mNmtwmm2/I7nvtyhNTn+Kaf10GwB9O+iN33nZ3zpFZo/3svKuY+MR03np3Nl846g8cstv27LHdxvzzvinzNe8BDBywBN/80hZ848QLkWDb9T/FdhuulVPkvVNrJwYfSRoJjCwrGh0RoysmC+BmSUG6vGc0MDgiSsNwXwEG04MUHkHVMOusuJk3ts3noWuOyDsEK6B+W+/b7d/D3W/oHjUfb/40/eoOlyfpExHxkqQVSa1LhwLXR8QyZdO8GRHLdingKnpNE5+ZmdWup4eZR8RL2f+vkVqyPgO8KmllgOz/13pyHZygzMyaUHTiX0ckDZC0VOlv4IvAo8D1wP7ZZPuTBrP1mN7WB2VmZjXo4eubBgPXZJfWLAJcnvXR3w9cKekgYDqwV08u1AnKzKwJtfRgioqIZ4EFRitHxOvADj22oApOUGZmTajId4iolROUmVkTaoYR2k5QZmZNqMg3ga2VE5SZWRNyE5+ZmRVSTw6SyIsTlJlZE3IflJmZFVLvrz85QZmZNaWe/j2oPDhBmZk1IY/iMzOzQnIflJmZFZJH8ZmZWSF15gcLi8oJysysCfX+9OQEZWbWlDxIwszMCskJyszMCqklPEjCzMwKyBfqmplZIfk6KDMzKyT3QZmZWSG5BmVmZoXkGpSZmRWSR/GZmVkheRSfmZkVku/FZ2ZmheQalJmZFZJrUGZmVkiuQZmZWSF5FJ+ZmRVSOEGZmVkR+UJdMzMrJN/qyMzMCsk1KDMzK6SWVvdBmZlZAXmYuZmZFZL7oMzMrJDcB2VmZoXkGpSZmRWS78VnZmaF5FsdmZlZIbmJz8zMCslNfGZmVki+DsrMzArJNSgzMysk90GZmVkhtXoUn5mZFVEz1KDUDCthvY+kkRExOu84rDi8T1ilPnkHYAutkXkHYIXjfcLm4wRlZmaF5ARlZmaF5ARleXFfg1XyPmHz8SAJMzMrJNegzMyskJygzMyskJygzMyskJygzMyskJygrGEkDZZ0kaQbs+frSjoo77gsP5LWknSbpEez5xtIOjbvuKwYnKCskcYCNwFDsudPAkfkFo0VwQXAMcBHABExGdgn14isMJygrJFWiIgrgVaAiJgLtOQbkuWsf0TcV1E2N5dIrHCcoKyR3pO0PKSf+pS0BTAr35AsZzMlrcG8fWJP4OV8Q7Ki8IW61jCSNgHOBtYDHgUGAXtmzTq2EJK0OukOElsBbwLPAftFxLQ847JicIKyhpK0CLA2IOCJiPgo55CsACQNAPpExDt5x2LF4SY+axhJPwCWjIgpEfEosKSk7+cdl+VH0m8kLRMR70XEO5KWlfTrvOOyYnANyhpG0kMRsVFF2YMRsXFeMVm+qn3+kh6IiE3yismKwzUoa6S+klR6IqkvsFiO8Vj++kpavPRE0hLA4u1MbwuRRfIOwBYq/wT+LOn87PnBWZktvC4DbpM0Jnt+IDAux3isQNzEZw0jqQ8pKe2QFd0CXBgRvhZqISZpZ8r2iYi4Kc94rDicoMzMrJDcB2UNI2lrSbdIelLSs5Kek/Rs3nFZfiTtIekpSbMkvS3pHUlv5x2XFYNrUNYwkh4HjgQmUXaLo4h4PbegLFeSnga+EhGP5R2LFY8HSVgjzYqIG/MOwgrlVScna4trUNYwkk4G+gJXAx+UyiPigdyCslxJOhNYCbiW+feJq3MLygrDCcoaRtLtVYojIj7f8GCsEMqGl5eLiPh2w4OxwnGCMjOzQnIflDWUpF2BTwP9SmUR8av8IrI8SeoHHMSC+4RrUOZh5tY4ks4D9gYOJd3N/OvA0FyDsrxdSuqD+hJwB7AK4DuaG+AmPmsgSZMjYoOy/5cEboyIbfOOzfJRulls2T6xKHBXRGyRd2yWP9egrJHmZP/PljQE+AhYOcd4LH+l3wN7S9J6wNLAijnGYwXiPihrpBskLQP8HniA9DPfF+YbkuVstKRlgeOA64ElgV/mG5IVhZv4LBfZTyz0i4hZecdiZsXkBGV1J2mP9l73RZkLH0k/au/1iDi9UbFYcbmJzxrhK+28FqQ7S9jCZam8A7Dicw3KzMwKyaP4rGEkLS3pdEkTs8dpkpbOOy7Lj6RVJF0j6bXscZWkVfKOy4rBCcoa6WLSRZh7ZY+3gWr3YrOFxxjS6L0h2eNveJ+wjJv4rGEkPRQRG3VUZgsP7xPWHtegrJHmSNqm9ETS1sy7eNcWTq9L2k9S3+yxH+AfsDTANShrIEkbAeNIdwsQ8AZwQEQ8nGtglhtJQ4GzgS1JIzrvBg6LiOdzDcwKwQnKGk7SQICIeDvvWMysuJygrO4k7RcRf2rr4kxflLnwkfTTiPidpLNJNaf5RMRhOYRlBeMLda0R+mf/++JMK/mepLuBiXkHYsXlBGWNsLSkVSLihLwDscI4i3TT4JWBK4ErIuLBfEOyovEoPmuEIcB/JN0l6fuSBuUdkOUrIs6IiC2Bz5JG7V0s6XFJv5S0Zs7hWUG4D8oaQpKA7YB9gN2Bh4ErgKsjwr+gakjamHQx9wYR0TfveCx/TlDWcJL6AjsCJwNrR0T/Dt5iTUrSIsDOpBOXHYAJpOa+6/KMy4rBfVDWUJLWJx2M9gZmAsfkG5HlQdIXgBHALsB9wHhgZES8l2tgViiuQVndZX0K+2SPFtLBaHxEPJtrYJYbSf8CLgeuiog3847HiskJyupO0jOk/qbxEfFo3vGYWe/gBGVmZoXkYebWMJL2kPSUpFmS3pb0jiTf7sjMqnINyhpG0tPAVyLisbxjMbPicw3KGulVJyczq5VrUNYwks4EVgKuBT4olUfE1bkFZWaF5eugrJEGArOBL5aVBeAEZWYLcA3KzMwKyX1Q1jCSVpF0jaTXssdVklbJOy4zKyYnKGukMcD1pLubDwH+lpWZmS3ATXzWMJIeioiNOiozMwPXoKyxXpe0n6S+2WM/0m8BmZktwDUoaxhJQ4GzgS1Jo/fuBg6LiOdzDczMCskJyszMCsnXQVndSfplOy9HRJzYsGDMrNdwDcrqTtJRVYoHAAcBy0fEkg0Oycx6AScoayhJSwGHk5LTlcBpEfFavlGZWRG5ic8aQtJywI+AfYFxwCb+JVUza48TlNWdpN8DewCjgfUj4t2cQzKzXsBNfFZ3klpJdy+fSxpe/vFLpEESA3MJzMwKzQnKzMwKyXeSMDOzQnKCMjOzQnKCsqYnqUXSQ5IelfQXSf27Ma+xkvbM/r5Q0rrtTLu9pK26sIxpklZo47WNJIWknWqYzwGShpQ9bzdes6JxgrKFwZyI2Cgi1gM+BL5X/qKkLo1mjYjvRMTUdibZHuh0gurACODf2f8dOYD0syZATfGaFYoTlC1s7gI+ldVu7pJ0PTA1u7v67yXdL2mypIMBlJwj6QlJtwIrlmYkaYKk4dnfO0l6QNLDkm6TNIyUCI/Mam/bShqU/Ujj/dlj6+y9y0u6WdIUSReSRjcuQJKAr5MSzxck9St77WeSHsmWf3JWyxsOXJYtf4mKeEdk0z8q6ZSy+bwr6aRsPvdIGtxTG96ss5ygbKGR1ZR2Bh7JijYBDo+ItUh3tpgVEZsBmwHflbQa8DVgbWBd4FtUqRFJGgRcAPxvRGwIfD0ipgHnAX/Iam93AWdmzzcD/he4MJvF8cC/I+LTwDXAqm2swlbAcxHxDDAB2DVb/s7AbsDm2fJ/FxF/BSYC+2bLn1MW7xDgFODzwEbAZpJ2z14eANyTzedO4LsdbFazuvGFurYwWELSQ9nfdwEXkQ7290XEc1n5F4ENSv1LwNLAmsB2wBUR0QL8V9K/qsx/C+DO0rwi4o024tgRWDdVhAAYKGnJbBl7ZO/9u6S27rAxAhif/T2elDCvyuY7JiJmd7D8ks2ACRExA0DSZVkM15KaQG/IppsEfKGDeZnVjROULQzmVPklX4D3youAQyPiporpdunBOPoAW0TE+1ViaZekvqRa126SfkGKd/ns3oY96aOYd3FkCz5GWI7cxGeW3AQcImlRAElrSRpAaubaO+ujWhn4XJX33gNslzUJlu47CPAOUJ5AbgYOLT2RVEqadwLfyMp2BpatsowdgMkR8cmIGBYRQ0m1p68BtwAHlkYntrP8kvuAz0paIUt8I4A72tguZrlxgjJLLgSmAg9IehQ4n1R7uAZ4KnvtEuA/lW/MmspGAldLehj4c/bS34CvlQZJAIcBw7NBGFOZN5rwBFKCm0Jq6qv2C8MjsljKXQWMiIh/AtcDE7OmzB9nr48FzisNkiiL92XgaOB24GFgUkRcV8M2Mmso3+rIzMwKyTUoMzMrJCcoMzMrJCcoMzMrJCcoMzMrJCcoMzMrJCcoMzMrJCcoMzMrJCcoMzMrpP8Pu6QEBGzG9BwAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "actual, predicted = get_actual_predicted_labels(test_ds)\n",
        "plot_confusion_matrix(actual, predicted, label_names, 'test')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddQG9sYxa1Ib"
      },
      "source": [
        "## Next steps\n",
        "\n",
        "Now that you have some familiarity with the MoViNet model and how to leverage various TensorFlow APIs (for example, for transfer learning), try using the code in this tutorial with your own dataset. The data does not have to be limited to video data. Volumetric data, such as MRI scans, can also be used with 3D CNNs. The NUSDAT and IMH datasets mentioned in [Brain MRI-based 3D Convolutional Neural Networks for Classification of Schizophrenia and Controls](https://arxiv.org/pdf/2003.08818.pdf) could be two such sources for MRI data.\n",
        "\n",
        "In particular, using the `FrameGenerator` class used in this tutorial and the other video data and classification tutorials will help you load data into your models.\n",
        "\n",
        "To learn more about working with video data in TensorFlow, check out the following tutorials:\n",
        "\n",
        "* [Load video data](https://www.tensorflow.org/tutorials/load_data/video)\n",
        "* [Build a 3D CNN model for video classification](https://www.tensorflow.org/tutorials/video/video_classification)\n",
        "* [MoViNet for streaming action recognition](https://www.tensorflow.org/hub/tutorials/movinet)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}